"""Scraping data for multiple symbols

This file has a list of symbols of NIFTY500 companies

Kindly be generous and don't scrape data for more than 20-30 symbols at a time

Random proxies are generated by fake-headers library to avoid IP getting blocked by Yahoo
"""

import os
import numpy as np
import pandas as pd
import requests
from lxml import html
from fake_headers import Headers

symbol = ['3MINDIA', 'ABB', 'ACC', 'AIAENG', 'APLAPOLLO', 'AUBANK', 'AARTIIND', 'AAVAS', 'ABBOTINDIA', 'ADANIGAS',
          'ADANIGREEN', 'ADANIPORTS', 'ADANIPOWER', 'ADANITRANS', 'ABCAPITAL', 'ABFRL', 'ADVENZYMES', 'AEGISCHEM',
          'AFFLE', 'AJANTPHARM', 'AKZOINDIA', 'APLLTD', 'ALKEM', 'ALKYLAMINE', 'ALLCARGO', 'AMARAJABAT', 'AMBER',
          'AMBUJACEM', 'APOLLOHOSP', 'APOLLOTYRE', 'ARVINDFASN', 'ASAHIINDIA', 'ASHOKLEY', 'ASHOKA', 'ASIANPAINT',
          'ASTERDM', 'ASTRAZEN', 'ASTRAL', 'ATUL', 'AUROPHARMA', 'AVANTIFEED', 'DMART', 'AXISBANK', 'BASF', 'BEML',
          'BSE', 'BAJAJ-AUTO', 'BAJAJCON', 'BAJAJELEC', 'BAJFINANCE', 'BAJAJFINSV', 'BAJAJHLDNG', 'BALKRISIND',
          'BALMLAWRIE', 'BALRAMCHIN', 'BANDHANBNK', 'BANKBARODA', 'BANKINDIA', 'MAHABANK', 'BATAINDIA', 'BAYERCROP',
          'BERGEPAINT', 'BDL', 'BEL', 'BHARATFORG', 'BHEL', 'BPCL', 'BHARATRAS', 'BHARTIARTL', 'INFRATEL', 'BIOCON',
          'BIRLACORPN', 'BSOFT', 'BLISSGVS', 'BLUEDART', 'BLUESTARCO', 'BBTC', 'BOMDYEING', 'BOSCHLTD', 'BRIGADE',
          'BRITANNIA', 'CARERATING', 'CCL', 'CESC', 'CRISIL', 'CSBBANK', 'CADILAHC', 'CANFINHOME', 'CANBK',
          'CAPLIPOINT', 'CGCL', 'CARBORUNIV', 'CASTROLIND', 'CEATLTD', 'CENTRALBK', 'CDSL', 'CENTURYPLY', 'CENTURYTEX',
          'CERA', 'CHAMBLFERT', 'CHENNPETRO', 'CHOLAHLDNG', 'CHOLAFIN', 'CIPLA', 'CUB', 'COALINDIA', 'COCHINSHIP',
          'COLPAL', 'CONCOR', 'COROMANDEL', 'CREDITACC', 'CROMPTON', 'CUMMINSIND', 'CYIENT', 'DBCORP', 'DCBBANK',
          'DCMSHRIRAM', 'DLF', 'DABUR', 'DALBHARAT', 'DEEPAKNTR', 'DELTACORP', 'DHANUKA', 'DBL', 'DISHTV', 'DCAL',
          'DIVISLAB', 'DIXON', 'LALPATHLAB', 'DRREDDY', 'EIDPARRY', 'EIHOTEL', 'ESABINDIA', 'EDELWEISS', 'EICHERMOT',
          'ELGIEQUIP', 'EMAMILTD', 'ENDURANCE', 'ENGINERSIN', 'EQUITAS', 'ERIS', 'ESCORTS', 'ESSELPACK', 'EXIDEIND',
          'FDC', 'FEDERALBNK', 'FINEORG', 'FINCABLES', 'FINPIPE', 'FSL', 'FORTIS', 'FCONSUMER', 'FRETAIL', 'GAIL',
          'GEPIL', 'GET&D', 'GHCL', 'GMMPFAUDLR', 'GMRINFRA', 'GALAXYSURF', 'GRSE', 'GARFIBRES', 'GICRE', 'GILLETTE',
          'GLAXO', 'GLENMARK', 'GODFRYPHLP', 'GODREJAGRO', 'GODREJCP', 'GODREJIND', 'GODREJPROP', 'GRANULES',
          'GRAPHITE', 'GRASIM', 'GESHIP', 'GREAVESCOT', 'GRINDWELL', 'GUJALKALI', 'FLUOROCHEM', 'GUJGASLTD', 'GMDCLTD',
          'GNFC', 'GPPL', 'GSFC', 'GSPL', 'GULFOILLUB', 'HEG', 'HCLTECH', 'HDFCAMC', 'HDFCBANK', 'HDFCLIFE', 'HFCL',
          'HATHWAY', 'HATSUN', 'HAVELLS', 'HEIDELBERG', 'HERITGFOOD', 'HEROMOTOCO', 'HEXAWARE', 'HSCL', 'HIMATSEIDE',
          'HINDALCO', 'HAL', 'HINDCOPPER', 'HINDPETRO', 'HINDUNILVR', 'HINDZINC', 'HONAUT', 'HUDCO', 'HDFC',
          'ICICIBANK', 'ICICIGI', 'ICICIPRULI', 'ISEC', 'ICRA', 'IDBI', 'IDFCFIRSTB', 'IDFC', 'IFBIND', 'IFCI', 'IIFL',
          'IIFLWAM', 'IRB', 'IRCON', 'ITC', 'ITI', 'INDIACEM', 'ITDC', 'IBULHSGFIN', 'IBREALEST', 'IBVENTURES',
          'INDIAMART', 'INDIANB', 'IEX', 'INDHOTEL', 'IOC', 'IOB', 'IRCTC', 'INDOSTAR', 'INDOCO', 'IGL', 'INDUSINDBK',
          'INFIBEAM', 'NAUKRI', 'INFY', 'INGERRAND', 'INOXLEISUR', 'INTELLECT', 'INDIGO', 'IPCALAB', 'JBCHEPHARM',
          'JKCEMENT', 'JKLAKSHMI', 'JKPAPER', 'JKTYRE', 'JMFINANCIL', 'JSWENERGY', 'JSWSTEEL', 'JAGRAN', 'JAICORPLTD',
          'J&KBANK', 'JAMNAAUTO', 'JINDALSAW', 'JSLHISAR', 'JSL', 'JINDALSTEL', 'JCHAC', 'JUBLFOOD', 'JUBILANT',
          'JUSTDIAL', 'JYOTHYLAB', 'KPRMILL', 'KEI', 'KNRCON', 'KPITTECH', 'KRBL', 'KSB', 'KAJARIACER', 'KALPATPOWR',
          'KANSAINER', 'KTKBANK', 'KARURVYSYA', 'KSCL', 'KEC', 'KOLTEPATIL', 'KOTAKBANK', 'L&TFH', 'LTTS', 'LICHSGFIN',
          'LAOPALA', 'LAXMIMACH', 'LTI', 'LT', 'LAURUSLABS', 'LEMONTREE', 'LINDEINDIA', 'LUPIN', 'LUXIND', 'MASFIN',
          'MMTC', 'MOIL', 'MRF', 'MGL', 'MAHSCOOTER', 'MAHSEAMLES', 'M&MFIN', 'M&M', 'MAHINDCIE', 'MHRIL', 'MAHLOG',
          'MANAPPURAM', 'MRPL', 'MARICO', 'MARUTI', 'MFSL', 'METROPOLIS', 'MINDTREE', 'MINDACORP', 'MINDAIND',
          'MIDHANI', 'MOTHERSUMI', 'MOTILALOFS', 'MPHASIS', 'MCX', 'MUTHOOTFIN', 'NATCOPHARM', 'NBCC', 'NCC', 'NESCO',
          'NHPC', 'NIITTECH', 'NLCINDIA', 'NMDC', 'NTPC', 'NH', 'NATIONALUM', 'NFL', 'NBVENTURES', 'NAVINFLUOR',
          'NESTLEIND', 'NILKAMAL', 'NAM-INDIA', 'OBEROIRLTY', 'ONGC', 'OIL', 'OMAXE', 'OFSS', 'ORIENTCEM', 'ORIENTELEC',
          'ORIENTREF', 'PIIND', 'PNBHOUSING', 'PNCINFRA', 'PTC', 'PVR', 'PAGEIND', 'PERSISTENT', 'PETRONET', 'PFIZER',
          'PHILIPCARB', 'PHOENIXLTD', 'PIDILITIND', 'PEL', 'POLYMED', 'POLYCAB', 'PFC', 'POWERGRID', 'PRAJIND',
          'PRESTIGE', 'PRSMJOHNSN', 'PGHL', 'PGHH', 'PNB', 'QUESS', 'RBLBANK', 'RECLTD', 'RITES', 'RADICO', 'RVNL',
          'RAIN', 'RAJESHEXPO', 'RALLIS', 'RCF', 'RATNAMANI', 'RAYMOND', 'REDINGTON', 'RELAXO', 'RELIANCE', 'REPCOHOME',
          'SBILIFE', 'SJVN', 'SKFINDIA', 'SRF', 'SADBHAV', 'SANOFI', 'SCHAEFFLER', 'SCHNEIDER', 'SIS', 'SEQUENT', 'SFL',
          'SCI', 'SHOPERSTOP', 'SHREECEM', 'RENUKA', 'SHRIRAMCIT', 'SRTRANSFIN', 'SIEMENS', 'SOBHA', 'SOLARINDS',
          'SONATSOFTW', 'SOUTHBANK', 'SPANDANA', 'SPICEJET', 'STARCEMENT', 'SBIN', 'SAIL', 'SWSOLAR', 'STRTECH', 'STAR',
          'SUDARSCHEM', 'SUMICHEM', 'SPARC', 'SUNPHARMA', 'SUNTV', 'SUNDARMFIN', 'SUNDRMFAST', 'SUNTECK', 'SUPRAJIT',
          'SUPREMEIND', 'SUZLON', 'SWANENERGY', 'SYMPHONY', 'SYNGENE', 'TCIEXP', 'TCNSBRANDS', 'TTKPRESTIG', 'TVTODAY',
          'TV18BRDCST', 'TVSMOTOR', 'TAKE', 'TASTYBITE', 'TATACOMM', 'TCS', 'TATACONSUM', 'TATAELXSI', 'TATAINVEST',
          'TATAMTRDVR', 'TATAMOTORS', 'TATAPOWER', 'TATASTLBSL', 'TATASTEEL', 'TEAMLEASE', 'TECHM', 'NIACL', 'RAMCOCEM',
          'THERMAX', 'THYROCARE', 'TIMETECHNO', 'TIMKEN', 'TITAN', 'TORNTPHARM', 'TORNTPOWER', 'TRENT', 'TRIDENT',
          'TIINDIA', 'UCOBANK', 'UFLEX', 'UPL', 'UJJIVAN', 'UJJIVANSFB', 'ULTRACEMCO', 'UNIONBANK', 'UBL', 'MCDOWELL-N',
          'VGUARD', 'VMART', 'VIPIND', 'VRLLOG', 'VSTIND', 'VAIBHAVGBL', 'VAKRANGEE', 'VTL', 'VARROC', 'VBL', 'VEDL',
          'VENKEYS', 'VESUVIUS', 'VINATIORGA', 'IDEA', 'VOLTAS', 'WABCOINDIA', 'WELCORP', 'WELSPUNIND', 'WESTLIFE',
          'WHIRLPOOL', 'WIPRO', 'WOCKPHARMA', 'ZEEL', 'ZENSARTECH', 'ZYDUSWELL', 'ECLERX']


def dir_create():
    # base dir
    _dir = "/path/to/save/folder/"

    # create dynamic name
    _create = os.path.join(_dir, symbol)

    # create 'dynamic' dir, if it does not exist
    if not os.path.exists(_create):
        os.makedirs(_create)


dir_create()


def sp_headers():
    if __name__ == "__main__":
        header = Headers(
            browser="chrome",  # Generate only Chrome UA
            os="win",  # Generate ony Windows platform
            headers=True  # generate misc headers
        )

        for i in range(10):
            header.generate()


def scrape():
    # Scraping Cash Flow
    def cash_flow():
        url_cf = f'https://in.finance.yahoo.com/quote/{symbol}.NS/cash-flow?p={symbol}.NS'
        # Fetch the page that we're going to parse, using the request headers
        # defined above
        page = requests.get(url_cf, headers=sp_headers())

        # Parse the page with LXML, so that we can start doing some XPATH queries
        # to extract the data that we want
        tree = html.fromstring(page.content)

        # Smoke test that we fetched the page by fetching and displaying the H1 element
        y_axis = tree.xpath("//h1/text()")
        print(y_axis)

        table_rows = tree.xpath("//div[contains(@class, 'D(tbr)')]")

        # Ensure that some table rows are found; if none are found, then it's possible
        # that Yahoo Finance has changed their page layout, or have detected
        # that you're scraping the page.
        assert len(table_rows) > 0

        parsed_rows = []

        for table_row in table_rows:
            parsed_row = []
            el = table_row.xpath("./div")

            none_count = 0

            for rs in el:
                try:
                    (text,) = rs.xpath('.//span/text()[1]')
                    parsed_row.append(text)
                except ValueError:
                    parsed_row.append(np.NaN)
                    none_count += 1

            if (none_count < 4):
                parsed_rows.append(parsed_row)

        df = pd.DataFrame(parsed_rows)
        df

        path = f'/path/to/save/folder/{symbol}'
        sheet_name = str(f'cash_flow_{symbol}.csv')
        writer = pd.ExcelWriter(sheet_name)
        writer = os.path.join(path, writer)
        df.to_excel(writer)

    cash_flow()

    def income_statement():
        url_is = f'https://in.finance.yahoo.com/quote/{symbol}.NS/financials?p={symbol}.NS'
        # Fetch the page that we're going to parse, using the request headers
        # defined above
        page = requests.get(url_is, headers=sp_headers())

        # Parse the page with LXML, so that we can start doing some XPATH queries
        # to extract the data that we want
        tree = html.fromstring(page.content)

        # Smoke test that we fetched the page by fetching and displaying the H1 element
        y_axis = tree.xpath("//h1/text()")
        print(y_axis)

        table_rows = tree.xpath("//div[contains(@class, 'D(tbr)')]")

        # Ensure that some table rows are found; if none are found, then it's possible
        # that Yahoo Finance has changed their page layout, or have detected
        # that you're scraping the page.
        assert len(table_rows) > 0

        parsed_rows = []

        for table_row in table_rows:
            parsed_row = []
            el = table_row.xpath("./div")

            none_count = 0

            for rs in el:
                try:
                    (text,) = rs.xpath('.//span/text()[1]')
                    parsed_row.append(text)
                except ValueError:
                    parsed_row.append(np.NaN)
                    none_count += 1

            if (none_count < 4):
                parsed_rows.append(parsed_row)

        df = pd.DataFrame(parsed_rows)
        df

        path = f'/path/to/save/folder/{symbol}'
        sheet_name = str(f'income_statement_{symbol}.csv')
        writer = pd.ExcelWriter(sheet_name)
        writer = os.path.join(path, writer)
        df.to_excel(writer)

    income_statement()

    def balance_sheet():
        url_bs = f'https://in.finance.yahoo.com/quote/{symbol}.NS/balance-sheet?p={symbol}.NS'
        # Fetch the page that we're going to parse, using the request headers
        # defined above
        page = requests.get(url_bs, headers=sp_headers())

        # Parse the page with LXML, so that we can start doing some XPATH queries
        # to extract the data that we want
        tree = html.fromstring(page.content)

        # Smoke test that we fetched the page by fetching and displaying the H1 element
        y_axis = tree.xpath("//h1/text()")
        print(y_axis)

        table_rows = tree.xpath("//div[contains(@class, 'D(tbr)')]")

        # Ensure that some table rows are found; if none are found, then it's possible
        # that Yahoo Finance has changed their page layout, or have detected
        # that you're scraping the page.
        assert len(table_rows) > 0

        parsed_rows = []

        for table_row in table_rows:
            parsed_row = []
            el = table_row.xpath("./div")

            none_count = 0

            for rs in el:
                try:
                    (text,) = rs.xpath('.//span/text()[1]')
                    parsed_row.append(text)
                except ValueError:
                    parsed_row.append(np.NaN)
                    none_count += 1

            if (none_count < 4):
                parsed_rows.append(parsed_row)

        df = pd.DataFrame(parsed_rows)
        df

        path = f'/path/to/save/folder/{symbol}'
        sheet_name = str(f'balance_sheet_{symbol}.csv')
        writer = pd.ExcelWriter(sheet_name)
        writer = os.path.join(path, writer)
        df.to_excel(writer)

    balance_sheet()


for sym in symbol:
    dir_create(sym)
    scrape(sym)
